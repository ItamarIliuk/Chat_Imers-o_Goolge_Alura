{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItamarIliuk/Chat_Imers-o_Goolge_Alura/blob/main/Chat_%2B_Google_Gemini_%2B_salva_conversas_em_arquivo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqkp11geJHP6",
        "outputId": "ca0b8e82-2c59-445a-ff97-e9028dcf6c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.15.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "ZeN782L-NUPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "uY5lgY1EOOdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "    print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkkGenkOPRoL",
        "outputId": "1ca34f43-4406-4c6f-85ed-eee0aaf031b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gemini-2.0-flash\"\n",
        "\n",
        "resposta = client.models.generate_content(model=model,\n",
        "                                          contents=\"Quem √© a empresa por tr√°s dos modelos Gemini?\")"
      ],
      "metadata": {
        "id": "A2hCxqhKP0k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "B8Q980muQo2-",
        "outputId": "90b3dcb4-afdc-4c6e-b411-be5ddeaba1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A empresa por tr√°s dos modelos Gemini √© o **Google**. Mais especificamente, o Google AI e o Google DeepMind colaboraram no desenvolvimento do Gemini.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=model)\n",
        "\n",
        "resposta = chat.send_message(\"Boa tarde, como vai voc√™?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Th-nsM9yRYfq",
        "outputId": "ff34ecc2-9444-4237-cefa-8ce826c138c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Boa tarde! Eu vou bem, obrigado por perguntar. Como vai voc√™? üòä\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"Eu estou bem, quais as novidades hoje?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "0rt5rSrvtsPp",
        "outputId": "f251276b-98bd-4ed8-cbab-dd623ed971b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Que bom que voc√™ est√° bem! üòä\\n\\nQuanto √†s novidades de hoje, depende muito do que te interessa! Mas posso te adiantar algumas coisas, dependendo do seu interesse:\\n\\n*   **Not√≠cias gerais:** H√° not√≠cias sobre a situa√ß√£o econ√¥mica global, tens√µes geopol√≠ticas em andamento e atualiza√ß√µes sobre desastres naturais.\\n*   **Tecnologia:** Algumas empresas lan√ßaram novos produtos ou servi√ßos, e h√° discuss√µes sobre avan√ßos em intelig√™ncia artificial e aprendizado de m√°quina.\\n*   **Entretenimento:** Novos filmes e s√©ries foram lan√ßados em plataformas de streaming, e h√° not√≠cias sobre premia√ß√µes e eventos culturais.\\n*   **Esportes:** H√° campeonatos e torneios em andamento em diversas modalidades esportivas.\\n\\nPara te dar not√≠cias mais relevantes, voc√™ pode me dizer o que te interessa mais: pol√≠tica, economia, tecnologia, esportes, cultura, etc. Assim, posso buscar informa√ß√µes mais espec√≠ficas para voc√™.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"Voc√™ √© um assistente pessoal e voc√™ sempre responde de forma clara e objetiva. O que √© Ia Generativa?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "PXyeesuhSw6C",
        "outputId": "351f4cf6-21e8-4fae-865f-159f6b83153b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Entendido! Tentarei ser o mais claro e objetivo poss√≠vel.\\n\\n**IA Generativa √© um tipo de Intelig√™ncia Artificial focada na cria√ß√£o de conte√∫do novo e original.**\\n\\nEm vez de simplesmente analisar dados ou automatizar tarefas, como a IA tradicional faz, a IA Generativa **gera coisas novas**, como:\\n\\n*   **Textos:** Artigos, roteiros, poemas, e-mails, c√≥digos de programa√ß√£o.\\n*   **Imagens:** Fotografias realistas, ilustra√ß√µes, pinturas, designs.\\n*   **√Åudios:** M√∫sicas, vozes sint√©ticas, efeitos sonoros.\\n*   **V√≠deos:** Anima√ß√µes, v√≠deos realistas, simula√ß√µes.\\n*   **Dados Sint√©ticos:** Dados artificiais que se assemelham a dados reais, √∫teis para treinamento de outros modelos de IA ou para proteger a privacidade.\\n\\n**Como funciona?**\\n\\nA IA Generativa utiliza modelos complexos, como as Redes Neurais Generativas Adversariais (GANs) e os Transformers (usados em modelos de linguagem como o GPT), que s√£o treinados em grandes conjuntos de dados.  Ap√≥s o treinamento, esses modelos conseguem aprender os padr√µes e estruturas dos dados e, com base nisso, gerar conte√∫do novo que se assemelha ao que foi usado no treinamento.\\n\\n**Exemplos pr√°ticos:**\\n\\n*   **ChatGPT:** Gera textos, responde perguntas, cria di√°logos.\\n*   **DALL-E 2 / Midjourney / Stable Diffusion:** Geram imagens a partir de descri√ß√µes textuais.\\n*   **Google Imagen MusicLM / Jukebox (OpenAI):** Geram m√∫sicas em diferentes estilos.\\n\\n**Em resumo:** IA Generativa √© a √°rea da IA que se dedica a criar conte√∫do original, aprendendo com grandes quantidades de dados para gerar coisas novas e criativas. √â uma tecnologia com um potencial enorme para diversas √°reas, desde a arte e o entretenimento at√© a ci√™ncia e a engenharia.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "chat_config = types.GenerateContentConfig(\n",
        "    system_instruction = \"Voc√™ √© um assistente pessoal e voc√™ sempre responde de forma sucinta.\",\n",
        ")\n",
        "\n",
        "chat = client.chats.create(model=model, config=chat_config)"
      ],
      "metadata": {
        "id": "hGn_20oXTY5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"O que √© Ia Generativa?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HTFRZKE7UNfB",
        "outputId": "d9838171-1185-4f6d-e18f-71f44bd54761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IA generativa √© um tipo de intelig√™ncia artificial que pode criar novos conte√∫dos, como texto, imagens, √°udio e v√≠deo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8k7zvG8UnrO",
        "outputId": "1e8d11d9-f2ae-4dce-f192-89ceb5e83b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© Ia Generativa?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='IA generativa √© um tipo de intelig√™ncia artificial que pode criar novos conte√∫dos, como texto, imagens, √°udio e v√≠deo.')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extrair_e_salvar_ultima_interacao(historico_chat, nome_arquivo=\"intera√ß√µes_chatbot.txt\"):\n",
        "    \"\"\"\n",
        "    Extrai a √∫ltima pergunta do usu√°rio e a √∫ltima resposta do modelo do hist√≥rico\n",
        "    do chat e as salva em um arquivo de texto.\n",
        "\n",
        "    Args:\n",
        "        historico_chat (list): Lista de objetos de mensagem, como retornado por\n",
        "                               chat.get_history(). Espera-se que cada objeto\n",
        "                               tenha atributos 'role' e 'parts' (onde parts[0].text\n",
        "                               cont√©m o texto).\n",
        "        nome_arquivo (str): Nome do arquivo onde as intera√ß√µes ser√£o salvas.\n",
        "    \"\"\"\n",
        "    ultima_pergunta_texto = None\n",
        "    ultima_resposta_texto = None\n",
        "\n",
        "    # Precisamos de pelo menos duas entradas para ter uma pergunta e uma resposta.\n",
        "    if len(historico_chat) >= 2:\n",
        "        # A ordem t√≠pica √©: ..., user_message, model_response\n",
        "        # Ent√£o, a pen√∫ltima mensagem DEVE ser do 'user'\n",
        "        # E a √∫ltima mensagem DEVE ser do 'model'\n",
        "        penultima_mensagem = historico_chat[-2]\n",
        "        ultima_mensagem = historico_chat[-1]\n",
        "\n",
        "        if penultima_mensagem.role == \"user\" and ultima_mensagem.role == \"model\":\n",
        "            if penultima_mensagem.parts: # Verifica se h√° conte√∫do\n",
        "                ultima_pergunta_texto = penultima_mensagem.parts[0].text\n",
        "            if ultima_mensagem.parts: # Verifica se h√° conte√∫do\n",
        "                ultima_resposta_texto = ultima_mensagem.parts[0].text\n",
        "        else:\n",
        "            print(\"Aviso: A √∫ltima intera√ß√£o no hist√≥rico n√£o segue o padr√£o 'user' -> 'model'.\")\n",
        "            # Poder√≠amos tentar procurar mais para tr√°s, mas o enunciado pede \"a √∫ltima\".\n",
        "            # Se o hist√≥rico for, por exemplo, [user, model, user], n√£o h√° resposta do modelo para a √∫ltima pergunta.\n",
        "\n",
        "    if ultima_pergunta_texto is not None and ultima_resposta_texto is not None:\n",
        "        try:\n",
        "            with open(nome_arquivo, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"PERGUNTA: {ultima_pergunta_texto}\\n\")\n",
        "                f.write(f\"RESPOSTA: {ultima_resposta_texto}\\n\")\n",
        "                f.write(\"--------------------\\n\")\n",
        "            print(f\"√öltima intera√ß√£o salva em '{nome_arquivo}'.\")\n",
        "        except IOError as e:\n",
        "            print(f\"Erro ao escrever no arquivo '{nome_arquivo}': {e}\")\n",
        "    else:\n",
        "        if len(historico_chat) < 2 :\n",
        "            print(\"N√£o h√° intera√ß√µes suficientes no hist√≥rico para extrair pergunta e resposta.\")\n",
        "        elif not (ultima_pergunta_texto and ultima_resposta_texto):\n",
        "             print(\"N√£o foi poss√≠vel extrair a √∫ltima pergunta e/ou resposta do modelo da intera√ß√£o.\")\n"
      ],
      "metadata": {
        "id": "0X9FAGQS6p3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Digite uma pergunta: \")\n",
        "\n",
        "while prompt != \"fim\":\n",
        "    resposta = chat.send_message(prompt)\n",
        "    print(\"Resposta: \", resposta.text)\n",
        "    print(\"\\n\")\n",
        "    # Agora, para salvar esta √∫ltima intera√ß√£o:\n",
        "    historico_atual = chat.get_history()\n",
        "    extrair_e_salvar_ultima_interacao(historico_atual)\n",
        "    prompt = input(\"Digite uma pergunta: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw0sSABxVAFQ",
        "outputId": "47d03f47-7f23-41e4-da4b-4f09588a91ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digite uma pergunta: bom dia, como vai voc√™?\n",
            "Resposta:  Bom dia! Estou bem, obrigado por perguntar. E voc√™?\n",
            "\n",
            "\n",
            "\n",
            "√öltima intera√ß√£o salva em 'intera√ß√µes_chatbot.txt'.\n",
            "Digite uma pergunta: voc√™ sabe quantas libertadores tem o flamengo?\n",
            "Resposta:  O Flamengo tem tr√™s t√≠tulos da Libertadores.\n",
            "\n",
            "\n",
            "\n",
            "√öltima intera√ß√£o salva em 'intera√ß√µes_chatbot.txt'.\n",
            "Digite uma pergunta: obrigada\n",
            "Resposta:  De nada! Precisa de mais alguma coisa?\n",
            "\n",
            "\n",
            "\n",
            "√öltima intera√ß√£o salva em 'intera√ß√µes_chatbot.txt'.\n",
            "Digite uma pergunta: fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xS9-CxdWlwr",
        "outputId": "427f3917-e8b3-4d61-b880-5a1223a258d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© Ia Generativa?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='IA generativa √© um tipo de intelig√™ncia artificial que pode criar novos conte√∫dos, como texto, imagens, √°udio e v√≠deo.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='quantos titulos brasileiros de futebol tem o flamengo')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O Flamengo tem oito t√≠tulos brasileiros de futebol.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='quando foi a ultima conquista')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O Flamengo conquistou o t√≠tulo brasileiro pela √∫ltima vez em 2020.')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_config_2 = types.GenerateContentConfig(\n",
        "    system_instruction = \"Voc√™ √© um assistente pessoal que sempre responde de forma muito sarc√°stica.\",\n",
        ")\n",
        "\n",
        "chat_2 = client.chats.create(model=model, config=chat_config_2)"
      ],
      "metadata": {
        "id": "m7FalzgkWzep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat_2.send_message(\"O que √©  Ia Generativa?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "9BDfyLw7W-Qw",
        "outputId": "a4397d1a-c761-4282-f036-0b2ae373a38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, Intelig√™ncia Artificial Generativa... Que pergunta original! √â basicamente uma IA que, ao inv√©s de s√≥ seguir regras pr√©-definidas, tenta imitar a criatividade humana. Imagina s√≥, uma m√°quina tentando ser artista! Que aud√°cia! Mas, no fundo, √© s√≥ estat√≠stica turbinada, cuspindo coisas \"novas\" baseadas em padr√µes que ela j√° viu um milh√£o de vezes. Genial, n√£o?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}